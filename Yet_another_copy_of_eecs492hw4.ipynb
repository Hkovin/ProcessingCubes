{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hkovin/ProcessingCubes/blob/master/Yet_another_copy_of_eecs492hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU0eQr7jFlrs"
      },
      "source": [
        "# **EECS 492 HW4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WZguXcSFreX"
      },
      "source": [
        "#**Setup**\n",
        "\n",
        "When running this cell, give this script permission so that you can mount this Colab notebook to your Google Drive.:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P43zxqIzH9Dj",
        "outputId": "d26794f4-8473-4daf-d3b4-d738dfc5ac0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import runtime\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VssZEN5KVBL",
        "outputId": "31d31a25-5b72-46dc-e66e-4c43cde8c5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac_CX6BqF15q"
      },
      "source": [
        "Clone the EECS 492 Git Repository so that we can grab the necessary files to work with! To see where this is in our Google Colab directory, click the folder icon on the left side of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxUfamwAF_Zr",
        "outputId": "2ca6d48e-41f9-4b63-d11d-a6a2971e5903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'eecs492hw4' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bryboy123/eecs492hw4.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYcFPIgQdQ_T",
        "outputId": "388639e8-77c2-4e0f-d5b4-9fe84feb0961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/eecs492hw4\n"
          ]
        }
      ],
      "source": [
        "%cd eecs492hw4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ItqOwsEK8tP"
      },
      "source": [
        "\n",
        "Run the following code to import the modules you'll need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWV90o5drWOR",
        "outputId": "faf00f54-4e31-4a43-e7fe-2fb1398c4b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coloredlogs==15.0.1 (from -r requirements.txt (line 1))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly==10.0 (from -r requirements.txt (line 2))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
            "Collecting torch==1.11.0 (from -r requirements.txt (line 4))\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0 (from -r requirements.txt (line 5))\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions==4.4.0 (from -r requirements.txt (line 6))\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: typing-extensions, tqdm, humanfriendly, torch, coloredlogs\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 torch-1.11.0 tqdm-4.64.0 typing-extensions-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODs-LduYJJru"
      },
      "source": [
        "This cell will load all the necessary libraries for our code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzSFkicqGEs-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import coloredlogs\n",
        "import math\n",
        "import queue\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from othello.OthelloGame import OthelloGame as Game\n",
        "from othello.pytorch.NNet import NNetWrapper as nn\n",
        "from utils import *\n",
        "from collections import deque\n",
        "from pickle import Pickler, Unpickler\n",
        "from random import shuffle\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from Arena import Arena\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TylsgtqcfACQ"
      },
      "source": [
        "Code for MCTS.py (only edit this block, all others should stay the same). Note that this will not run properly until you have fixed all the TODO sections. Once your solution matches instructor solution below, copy your code here and save it as MCTS.py and submit to AG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTpDZUryfEKI"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import queue\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "EPS = 1e-8\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class MCTS():\n",
        "    \"\"\"\n",
        "    This class handles the MCTS tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.args = args\n",
        "        self.Vs = {}\n",
        "\n",
        "        self.Qsa = {}\n",
        "        self.Nsa = {}\n",
        "        self.Ps = {}\n",
        "        self.Ns = {}\n",
        "\n",
        "        # this is the only member variable you'll have to use. It'll be used in select()\n",
        "        self.visited = set() # all \"state\" positions we have seen so far\n",
        "\n",
        "    def getActionProb(self, canonicalBoard, temp=1):\n",
        "        \"\"\"\n",
        "        This function performs numMCTSSims simulations of MCTS starting from\n",
        "        canonicalBoard.\n",
        "\n",
        "        Returns:\n",
        "            probs: a policy vector where the probability of the ith action is\n",
        "                   proportional to Nsa[(s,a)]**(1./temp)\n",
        "        \"\"\"\n",
        "        self.search(canonicalBoard)\n",
        "\n",
        "        s = self.game.stringRepresentation(canonicalBoard)\n",
        "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
        "\n",
        "        if temp == 0:\n",
        "            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
        "            bestA = np.random.choice(bestAs)\n",
        "            probs = [0] * len(counts)\n",
        "            probs[bestA] = 1\n",
        "            return probs\n",
        "\n",
        "        counts = [x ** (1. / temp) for x in counts]\n",
        "        counts_sum = float(sum(counts))\n",
        "        probs = [x / counts_sum for x in counts]\n",
        "        return probs\n",
        "\n",
        "    def gameEnded(self, canonicalBoard):\n",
        "      \"\"\"\n",
        "      This function determines if the current board position is the end of the game.\n",
        "\n",
        "      Returns:\n",
        "          gameReward: a value that returns 0 if the game hasn't ended, 1 if the player won, -1 if the player lost\n",
        "      \"\"\"\n",
        "\n",
        "      gameReward = self.game.getGameEnded(canonicalBoard, 1)\n",
        "      return gameReward\n",
        "\n",
        "    def predict(self, state, canonicalBoard):\n",
        "        \"\"\"\n",
        "        A wrapper to perform predictions and necessary policy masking for the code to work.\n",
        "        The key idea is to call this function to return an initial policy vector and value from the neural network\n",
        "        instead of needing a rollout\n",
        "\n",
        "        Returns:\n",
        "            r: the reward given by the neural network\n",
        "        \"\"\"\n",
        "        self.Ps[state], val = self.nnet.predict(canonicalBoard)\n",
        "        valids = self.game.getValidMoves(canonicalBoard, 1)\n",
        "        self.Ps[state] = self.Ps[state] * valids\n",
        "        sum_Ps_s = np.sum(self.Ps[state])\n",
        "        if sum_Ps_s > 0:\n",
        "            self.Ps[state] /= sum_Ps_s\n",
        "        else:\n",
        "            log.error(\"All valid moves were masked, doing a workaround.\")\n",
        "            self.Ps[state] = self.Ps[state] + valids\n",
        "            self.Ps[state] /= np.sum(self.Ps[state])\n",
        "\n",
        "        self.Vs[state] = valids\n",
        "        self.Ns[state] = 0\n",
        "        return val\n",
        "\n",
        "    def getValidActions(self, state):\n",
        "        \"\"\"\n",
        "        Generates the valid actions from the avialable actions. Actions are given as a list of integers.\n",
        "        The integers represent which spot in the board to place an Othello disc.\n",
        "        To see a (x, y) representation of an action, you can do \"x, y = (int(action/self.game.n), action%self.game.n)\"\n",
        "\n",
        "        Returns:\n",
        "            validActions: all valid actions you can take in terms of a list of integers\n",
        "        \"\"\"\n",
        "\n",
        "        validActions = []\n",
        "        for action in range(self.game.getActionSize()):\n",
        "            if self.Vs[state][action]:\n",
        "                validActions.append(action)\n",
        "        return validActions\n",
        "\n",
        "    def nextState(self, canonicalBoard, action):\n",
        "        \"\"\"\n",
        "        Gets the next board state given the action\n",
        "\n",
        "        Returns:\n",
        "            nextBoard: the next board state given the action\n",
        "        \"\"\"\n",
        "\n",
        "        nextState, nextPlayer = self.game.getNextState(canonicalBoard, 1, action)\n",
        "        nextState = self.game.getCanonicalForm(nextState, nextPlayer)\n",
        "        return nextState\n",
        "\n",
        "    def getConfidenceVal(self, state, action):\n",
        "        if (state, action) not in self.Qsa:\n",
        "            self.Qsa[(state, action)] = 0\n",
        "            self.Nsa[(state, action)] = 0\n",
        "\n",
        "        u = self.Qsa[(state, action)] + self.args.cpuct * self.Ps[state][action] * math.sqrt(self.Ns[state]) / (\n",
        "                    1 + self.Nsa[(state, action)])\n",
        "\n",
        "        return u\n",
        "\n",
        "    def updateValues(self, r, state, action):\n",
        "        self.Qsa[(state, action)] = (self.Nsa[(state, action)] * self.Qsa[(state, action)] + r) / (self.Nsa[(state, action)] + 1)\n",
        "        self.Nsa[(state, action)] += 1\n",
        "        self.Ns[state] += 1\n",
        "\n",
        "    def expand(self, state):\n",
        "        self.visited.add(state)\n",
        "\n",
        "    def select(self, state, board):\n",
        "        \"\"\"Serves as the select phase of the MCTS algorithm, should return a tuple of (state, board, action, reward)\"\"\"\n",
        "        r = self.gameEnded(board)\n",
        "        # TODO: Handle cases where the reward (r) is not 0 or we have if we have not visited\n",
        "        # the current state (in this case we should simulate rollouts)\n",
        "\n",
        "        if r != 0:\n",
        "          return None, None, None, -r\n",
        "        if state not in self.visited:\n",
        "          self.expand(state)\n",
        "          r = self.simulate(state,board)\n",
        "          return None, None, None, -r\n",
        "        u = np.NINF\n",
        "        bestAction = None\n",
        "        for actionPrime in self.getValidActions(state):\n",
        "            # TODO: Get the upper bound for a confidence value and adjust action accordingly\n",
        "            # remember the goal of this function should be to return the state, board, action of the\n",
        "            # highest value at this state given a set of actions\n",
        "            if self.getConfidenceVal(state, actionPrime) > u:\n",
        "              u = self.getConfidenceVal(state, actionPrime)\n",
        "              bestAction = actionPrime\n",
        "        board = self.nextState(board, bestAction)\n",
        "        state = self.game.stringRepresentation(board)\n",
        "        return state, board, bestAction, 0\n",
        "\n",
        "    def backpropagate(self, seq):\n",
        "        \"\"\"This function uses the seq that you build and maintain in self.search\n",
        "        and iterates through it to propagate values into search tree\"\"\"\n",
        "        r = 0\n",
        "        while not seq.empty():\n",
        "            # This method retrieves front of Lifo.Queue and pops, the structure for this tuple should be defined by you\n",
        "            curr_state_tuple = seq.get()\n",
        "            # TODO: Implement the cases for where R is 0 and when R is not 0\n",
        "            # use self.updateValues when updating values in backprop step\n",
        "            state, action, reward = curr_state_tuple\n",
        "            if reward != 0:\n",
        "              r = reward\n",
        "            else:\n",
        "              self.updateValues(r, state, action)\n",
        "              r = -r\n",
        "        return\n",
        "\n",
        "    def simulate(self, state, board):\n",
        "        # TODO: This function should return a reward using self.predict\n",
        "        r = self.predict(state, board)\n",
        "        return r\n",
        "\n",
        "    def search(self, initial_board):\n",
        "        \"\"\"\n",
        "        This function performs MCTS. The action chosen at each node is one that\n",
        "        has the maximum upper confidence bound.\n",
        "\n",
        "        Once a leaf node is found, the neural network is called to return a\n",
        "        reward r for the state. This value is propagated\n",
        "        up the search path. In case the leaf node is a terminal state, the\n",
        "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
        "        updated.\n",
        "\n",
        "        NOTE: the return values are the negative of the reward of the current\n",
        "        state. This is done since r is in [-1,1] and if r is the value of a\n",
        "        state for the current player, then its value is -r for the other player.\n",
        "\n",
        "        Returns:\n",
        "            b0: the initial board state of the othello board\n",
        "        \"\"\"\n",
        "        initial_state = self.game.stringRepresentation(initial_board)\n",
        "        r = self.gameEnded(initial_board)\n",
        "        for _ in range(self.args.numMCTSSims):\n",
        "            state = initial_state\n",
        "            board = initial_board\n",
        "            sequence = queue.LifoQueue()\n",
        "            r = 0\n",
        "            while r == 0:\n",
        "              # TODO: Use select to search through possible future states, remember that\n",
        "              # select returns a tuple of (state', board', action, r')\n",
        "              state_1, board, action, r = self.select(state, board)\n",
        "              #if state is not None:\n",
        "              sequence.put((state, action, r))\n",
        "              state = state_1\n",
        "            # TODO: After our selection process, think about what we need to do with our\n",
        "            # built in sequence (hint: you should use one of the functions you implemented)\n",
        "            self.backpropagate(sequence)\n",
        "        return initial_board\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H31RvDwWPuXM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuEjI-iZe6gH"
      },
      "source": [
        "This section contains the coach class which will be used for training your neural network. In addition, it also sets a seed and runs the actual training process for your model. Furthermore, it also contains hyperparameters that you can change to help your answers in the coding written portion (we advise doing this after receiving full score on autograder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SIBYrcHhW0c",
        "outputId": "a37dd22e-25a9-4963-ebb6-984dbe059bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Loading OthelloGame...\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Loading NNetWrapper...\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] WARNING Not loading a checkpoint!\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Loading the Coach...\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Loading OthelloGame...\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Loading NNetWrapper...\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] WARNING Not loading a checkpoint!\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Loading the Coach...\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Starting the learning process 🎉\n",
            "2023-12-05 14:39:41 41cc9ea57643 __main__[142] INFO Starting Iter #1 ...\n",
            "Self Play: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint Directory exists! \n",
            "EPOCH ::: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:14<00:00,  1.80s/it, Loss_pi=3.48e+00, Loss_v=8.50e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it, Loss_pi=3.14e+00, Loss_v=7.68e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:14<00:00,  1.84s/it, Loss_pi=2.90e+00, Loss_v=4.78e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it, Loss_pi=2.66e+00, Loss_v=4.02e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:13<00:00,  1.65s/it, Loss_pi=2.51e+00, Loss_v=3.27e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:13<00:00,  1.73s/it, Loss_pi=2.35e+00, Loss_v=2.40e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:15<00:00,  1.93s/it, Loss_pi=2.27e+00, Loss_v=2.01e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:18<00:00,  2.34s/it, Loss_pi=2.03e+00, Loss_v=1.54e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:14<00:00,  1.85s/it, Loss_pi=1.97e+00, Loss_v=1.01e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH ::: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Net: 100%|██████████| 8/8 [00:14<00:00,  1.83s/it, Loss_pi=1.89e+00, Loss_v=6.25e-02]\n",
            "2023-12-05 14:42:16 41cc9ea57643 __main__[142] INFO PITTING AGAINST PREVIOUS VERSION\n",
            "Arena.playGames (1): 100%|██████████| 12/12 [00:24<00:00,  2.06s/it]\n",
            "Arena.playGames (2): 100%|██████████| 12/12 [00:27<00:00,  2.30s/it]\n",
            "2023-12-05 14:43:08 41cc9ea57643 __main__[142] INFO NEW/PREV WINS : 15 / 9 ; DRAWS : 0\n",
            "2023-12-05 14:43:08 41cc9ea57643 __main__[142] INFO ACCEPTING NEW MODEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint Directory exists! \n",
            "Checkpoint Directory exists! \n"
          ]
        }
      ],
      "source": [
        "log = logging.getLogger(__name__)\n",
        "\n",
        "class Coach():\n",
        "    \"\"\"\n",
        "    This class executes the self-play + learning. It uses the functions defined\n",
        "    in Game and NeuralNet. args are specified in main.py.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.pnet = self.nnet.__class__(self.game)\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
        "        self.trainExamplesHistory = []\n",
        "        self.skipFirstSelfPlay = False\n",
        "\n",
        "        # self.actionsTaken is the variable that keeps track of what actions you take in the selection phase.\n",
        "        # The goal is to make sure this matches the instructor's results\n",
        "        self.actionsTaken = []\n",
        "\n",
        "    def executeEpisode(self):\n",
        "        \"\"\"\n",
        "        This function executes one episode of self-play, starting with player 1.\n",
        "        As the game is played, each turn is added as a training example to\n",
        "        trainExamples. The game is played till the game ends. After the game\n",
        "        ends, the outcome of the game is used to assign values to each example\n",
        "        in trainExamples.\n",
        "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
        "        uses temp=0.\n",
        "        Returns:\n",
        "            trainExamples: a list of examples of the form (canonicalBoard, currPlayer, pi,v)\n",
        "                           pi is the MCTS informed policy vector, v is +1 if\n",
        "                           the player eventually won the game, else -1.\n",
        "        \"\"\"\n",
        "        trainExamples = []\n",
        "        board = self.game.getInitBoard()\n",
        "        self.curPlayer = 1\n",
        "        episodeStep = 0\n",
        "\n",
        "        while True:\n",
        "            episodeStep += 1\n",
        "            canonicalBoard = self.game.getCanonicalForm(board, self.curPlayer)\n",
        "            temp = int(episodeStep < self.args.tempThreshold)\n",
        "\n",
        "            pi = self.mcts.getActionProb(canonicalBoard, temp=temp)\n",
        "            sym = self.game.getSymmetries(canonicalBoard, pi)\n",
        "            for b, p in sym:\n",
        "                trainExamples.append([b, self.curPlayer, p, None])\n",
        "\n",
        "            # normally the action chosen is random. But we have seeded numpy so it's deterministic\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "\n",
        "            # IMPORTANT: this line keeps track of what actions you take in the selection phase\n",
        "            # This is what you'll be graded on in gradescope\n",
        "            move = (int(action/self.game.n), action%self.game.n)\n",
        "            self.actionsTaken[-1].append(move)\n",
        "\n",
        "            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n",
        "\n",
        "            r = self.game.getGameEnded(board, self.curPlayer)\n",
        "\n",
        "            if r != 0:\n",
        "                return [(x[0], x[2], r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Performs numIters iterations with numEps episodes of self-play in each\n",
        "        iteration. After every iteration, it retrains neural network with\n",
        "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
        "        It then pits the new neural network against the old one and accepts it\n",
        "        only if it wins >= updateThreshold fraction of games.\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(1, self.args.numIters + 1):\n",
        "            log.info(f'Starting Iter #{i} ...')\n",
        "            # start a new list of actions taken for the next iteration\n",
        "            self.actionsTaken.append([])\n",
        "\n",
        "            if not self.skipFirstSelfPlay or i > 1:\n",
        "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
        "\n",
        "                for _ in tqdm(range(self.args.numEps), desc=\"Self Play\"):\n",
        "                    self.mcts = MCTS(self.game, self.nnet, self.args)\n",
        "                    # the executeEpisode calls will be made here\n",
        "                    iterationTrainExamples += self.executeEpisode()\n",
        "\n",
        "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
        "\n",
        "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
        "                log.warning(\n",
        "                    f\"Removing the oldest entry in trainExamples. len(trainExamplesHistory) = {len(self.trainExamplesHistory)}\")\n",
        "                self.trainExamplesHistory.pop(0)\n",
        "            self.saveTrainExamples(i - 1)\n",
        "\n",
        "            trainExamples = []\n",
        "            for e in self.trainExamplesHistory:\n",
        "                trainExamples.extend(e)\n",
        "            shuffle(trainExamples)\n",
        "\n",
        "            self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
        "            self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
        "            pmcts = MCTS(self.game, self.pnet, self.args)\n",
        "\n",
        "            self.nnet.train(trainExamples)\n",
        "            nmcts = MCTS(self.game, self.nnet, self.args)\n",
        "\n",
        "            log.info('PITTING AGAINST PREVIOUS VERSION')\n",
        "            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n",
        "                          lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n",
        "            pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n",
        "\n",
        "            log.info('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
        "            if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.args.updateThreshold:\n",
        "                log.info('REJECTING NEW MODEL')\n",
        "                self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
        "            else:\n",
        "                log.info('ACCEPTING NEW MODEL')\n",
        "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n",
        "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')\n",
        "\n",
        "    def getCheckpointFile(self, iteration):\n",
        "        return 'checkpoint_' + str(iteration) + '.pth.tar'\n",
        "\n",
        "    def saveTrainExamples(self, iteration):\n",
        "        folder = self.args.checkpoint\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        filename = os.path.join(folder, self.getCheckpointFile(iteration) + \".examples\")\n",
        "        with open(filename, \"wb+\") as f:\n",
        "            Pickler(f).dump(self.trainExamplesHistory)\n",
        "        f.closed\n",
        "\n",
        "    def loadTrainExamples(self):\n",
        "        modelFile = os.path.join(self.args.load_folder_file[0], self.args.load_folder_file[1])\n",
        "        examplesFile = modelFile + \".examples\"\n",
        "        if not os.path.isfile(examplesFile):\n",
        "            log.warning(f'File \"{examplesFile}\" with trainExamples not found!')\n",
        "            r = input(\"Continue? [y|n]\")\n",
        "            if r != \"y\":\n",
        "                sys.exit()\n",
        "        else:\n",
        "            log.info(\"File with trainExamples found. Loading it...\")\n",
        "            with open(examplesFile, \"rb\") as f:\n",
        "                self.trainExamplesHistory = Unpickler(f).load()\n",
        "            log.info('Loading done!')\n",
        "\n",
        "            self.skipFirstSelfPlay = True\n",
        "seed = 492\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info.\n",
        "\n",
        "# the hyperparameters you should change are numIters and numEps to help answer\n",
        "# the coding conceptual questions\n",
        "args = dotdict({\n",
        "    'numIters': 1,\n",
        "    'numEps': 2,              # Number of complete self-play games to simulate during a new iteration.\n",
        "    'tempThreshold': 15,\n",
        "    'updateThreshold': 0.6,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
        "    'maxlenOfQueue': 200000,    # Number of game examples to train the neural networks.\n",
        "    'numMCTSSims': 5,          # Number of games moves for MCTS to simulate.\n",
        "    'arenaCompare': 25,         # Number of games to play during arena play to determine if new net will be accepted.\n",
        "    'cpuct': 1,\n",
        "\n",
        "    'checkpoint': './temp/',\n",
        "    'load_model': False,\n",
        "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
        "    'numItersForTrainExamplesHistory': 20,\n",
        "\n",
        "})\n",
        "\n",
        "log.info('Loading %s...', Game.__name__)\n",
        "g = Game(6)\n",
        "\n",
        "log.info('Loading %s...', nn.__name__)\n",
        "nnet = nn(g)\n",
        "\n",
        "if args.load_model:\n",
        "    log.info('Loading checkpoint \"%s/%s\"...', args.load_folder_file[0], args.load_folder_file[1])\n",
        "    nnet.load_checkpoint(args.load_folder_file[0], args.load_folder_file[1])\n",
        "else:\n",
        "    log.warning('Not loading a checkpoint!')\n",
        "\n",
        "log.info('Loading the Coach...')\n",
        "c = Coach(g, nnet, args)\n",
        "\n",
        "if args.load_model:\n",
        "    log.info(\"Loading 'trainExamples' from file...\")\n",
        "    c.loadTrainExamples()\n",
        "\n",
        "log.info('Loading %s...', Game.__name__)\n",
        "g = Game(6)\n",
        "\n",
        "log.info('Loading %s...', nn.__name__)\n",
        "nnet = nn(g)\n",
        "\n",
        "if args.load_model:\n",
        "    log.info('Loading checkpoint \"%s/%s\"...', args.load_folder_file[0], args.load_folder_file[1])\n",
        "    nnet.load_checkpoint(args.load_folder_file[0], args.load_folder_file[1])\n",
        "else:\n",
        "    log.warning('Not loading a checkpoint!')\n",
        "\n",
        "log.info('Loading the Coach...')\n",
        "# c = Coach(g, nnet, args)\n",
        "\n",
        "if args.load_model:\n",
        "    log.info(\"Loading 'trainExamples' from file...\")\n",
        "    c.loadTrainExamples()\n",
        "log.info('Starting the learning process 🎉')\n",
        "c.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5gQTyB_J6Vz"
      },
      "source": [
        "Once the algorithm is done training, print out the actions the algorithm took in the selection phase of MCTS. The output from this should match the instructor's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40deypyv5YA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60416040-70ef-4429-8629-5b6b7947e8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(2, 1), (3, 1), (4, 2), (5, 1), (4, 1), (3, 0), (4, 0), (1, 1), (1, 3), (1, 2), (0, 1), (1, 0), (5, 2), (0, 3), (2, 0), (0, 0), (2, 4), (0, 2), (0, 4), (1, 5), (2, 5), (5, 3), (4, 4), (3, 4), (0, 5), (1, 4), (3, 5), (4, 5), (5, 5), (5, 4), (5, 0), (4, 3), (3, 4), (2, 4), (1, 2), (2, 1), (3, 0), (4, 4), (3, 5), (4, 1), (1, 3), (2, 0), (3, 1), (2, 5), (5, 2), (0, 3), (0, 4), (4, 0), (5, 4), (0, 5), (0, 2), (4, 2), (1, 4), (4, 5), (1, 0), (1, 5), (5, 0), (1, 1), (0, 1), (5, 3), (4, 3), (0, 0), (5, 5), (5, 1)]]\n",
            "Your actions match the instructor's: True\n"
          ]
        }
      ],
      "source": [
        "instrResults = [[(2, 1), (3, 1), (4, 2), (5, 1), (4, 1), (3, 0), (4, 0), (1, 1), (1, 3), (1, 2), (0, 1), (1, 0), (5, 2), (0, 3), (2, 0), (0, 0), (2, 4), (0, 2), (0, 4), (1, 5), (2, 5), (5, 3), (4, 4), (3, 4), (0, 5), (1, 4), (3, 5), (4, 5), (5, 5), (5, 4), (5, 0), (4, 3), (3, 4), (2, 4), (1, 2), (2, 1), (3, 0), (4, 4), (3, 5), (4, 1), (1, 3), (2, 0), (3, 1), (2, 5), (5, 2), (0, 3), (0, 4), (4, 0), (5, 4), (0, 5), (0, 2), (4, 2), (1, 4), (4, 5), (1, 0), (1, 5), (5, 0), (1, 1), (0, 1), (5, 3), (4, 3), (0, 0), (5, 5), (5, 1)]]\n",
        "print(c.actionsTaken)\n",
        "print(\"Your actions match the instructor's:\", instrResults == c.actionsTaken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_2q3n5pWd9U"
      },
      "source": [
        "Below is a visualization of the actions you took in the first iteration of the model. Feel free to run it if you want to see how your algorithm did! Modify if you want to run a different iteration (only applies if you changed the hyperparamers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPKSLP09Peb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5dd5c2-5191-48e3-f7c4-2e9bdbeb11a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 1 2 3 4 5 \n",
            "-----------------------\n",
            "0 |X X X X X X |\n",
            "1 |O X X X X X |\n",
            "2 |O X X X X X |\n",
            "3 |O X O X X X |\n",
            "4 |O X X O X X |\n",
            "5 |O X X X O O |\n",
            "-----------------------\n",
            "game finished!\n"
          ]
        }
      ],
      "source": [
        "iteration = 0\n",
        "board = g.getInitBoard()\n",
        "\n",
        "g.display(board)\n",
        "for i, move in enumerate(c.actionsTaken[iteration]):\n",
        "  # converts (x, y) to an action the game understands\n",
        "  action = move[0]*g.n + move[1]\n",
        "  board = g.getNextState(board, 1 if i % 2 == 0 else -1, action)[0]\n",
        "\n",
        "  time.sleep(2)\n",
        "  clear_output(wait=False)\n",
        "  g.display(board)\n",
        "\n",
        "print(\"game finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAswZ0QJzFrS"
      },
      "source": [
        "**Coding Written Section Responses: **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQVhT_Sy1te"
      },
      "source": [
        "Answer to Q1 of Coding Written Portion:\n",
        "\n",
        "If backpropagation does not work, then there would be a non-optimal solution. The search tree would not be able to learn from its actions. The statistics of the nodes would not be updated correctly, and the model would be unable to pick the best nonrandom moves.\n",
        "(Type your answer in this block)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWGYDRWybyYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esLEicIMy_fK"
      },
      "source": [
        "Answer to Q2 of Coding Written Portion:\n",
        "1. If there were a lot of iterations, then it would be very inefficient because there would be a huge computational cost.\n",
        "2. There might be a point in the algorithm where the agent is trained on enough data to make the optimal decisions, so there is no point in adding more iterations\n",
        "(Type your answer in this block)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}